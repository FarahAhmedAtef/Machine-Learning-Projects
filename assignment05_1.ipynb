{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSvGmsQcpk8F"
      },
      "source": [
        "# Assignment 5.1 - Model Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3kHmu4hpk8J"
      },
      "source": [
        "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
        "Please do **NOT** rename the file!\n",
        "\n",
        "#### State both names of your group members here:\n",
        "[Farah Ahmed Atef Abdelhameed Hafez - Mariz Essam Sobhy Ghaly]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "9x44v2AYpk8K"
      },
      "outputs": [],
      "source": [
        "##Farah Ahmed Atef Abdelhameed Hafez - Mariz Essam Sobhy Ghaly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CVDWaOrpk8L"
      },
      "source": [
        "---\n",
        "\n",
        "## Grading Info/Details - Assignment 5.1:\n",
        "\n",
        "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
        "\n",
        "* For passing the test scripts:\n",
        "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
        "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
        "\n",
        "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
        "\n",
        "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "VkejgytGpk8N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXvc__kpk8O"
      },
      "source": [
        "## Task 5.1.1 - Binary Classification Evaluation\n",
        "\n",
        "* Use model implementations of `sklearn` (or other) for Logistic Regression and SVM for classification tasks. Train both models on the `Breast Cancer` dataset. (see given imports) **(RESULT)**\n",
        "* Evaluate the performance of both models using appropriate classification metrics and implement them using `numpy` only. Report at least on the following: accuracy, precision, recall, F1-score. **(RESULT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and test the models"
      ],
      "metadata": {
        "id": "PVK31aoR4TlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "XIdVuDX5pk8P"
      },
      "outputs": [],
      "source": [
        "# Useful imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "KsKCSSQapk8R"
      },
      "outputs": [],
      "source": [
        "breastcancer = load_breast_cancer()\n",
        "X = breastcancer.data\n",
        "y = breastcancer.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "3AbI0iEkr0SA"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "e9X31PJct04-"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM= SVC()\n",
        "SVM.fit(X_train, y_train)\n",
        "SVM_pred = SVM.predict(X_test)"
      ],
      "metadata": {
        "id": "mL-m1J3Ut8e1"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LogReg= LogisticRegression()\n",
        "LogReg.fit(X_train, y_train)\n",
        "LogReg_pred = LogReg.predict(X_test)"
      ],
      "metadata": {
        "id": "Ke3pffduuAxK"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reporting Performance"
      ],
      "metadata": {
        "id": "Eb-DqBhROM8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a function that evaluates the models performance using basic metrics(Accuracy, Precision, Recall, f1_score)"
      ],
      "metadata": {
        "id": "pnNUDQ0t4izI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred):\n",
        "  classes= len(np.unique(y_true))\n",
        "  y_trueunique= np.unique(y_true)\n",
        "  if classes==2:\n",
        "    if y_trueunique[1]==1:\n",
        "      k=1\n",
        "      l=0\n",
        "    else:\n",
        "      k=0\n",
        "      l=1\n",
        "    TruePos = np.sum((y_true == y_trueunique[k]) & (y_pred == y_trueunique[k]))\n",
        "    FalsePos = np.sum((y_true == y_trueunique[l]) & (y_pred == y_trueunique[k]))\n",
        "    FalseNeg = np.sum((y_true == y_trueunique[k]) & (y_pred == y_trueunique[l]))\n",
        "    TrueNeg = np.sum((y_true == y_trueunique[l]) & (y_pred == y_trueunique[l]))\n",
        "    Accuracy= (TruePos + TrueNeg) / (TruePos + TrueNeg + FalsePos + FalseNeg)\n",
        "    Precision= TruePos / (TruePos + FalsePos)\n",
        "    Recall= TruePos / (TruePos + FalseNeg)\n",
        "    f1_score= 2/((1/Precision)+(1/Recall))\n",
        "    # MissRate= FalseNeg / (FalseNeg + TruePos)\n",
        "  else:\n",
        "    Precision, Recall, f1_score=0,0,0\n",
        "    for i in range(len(y_trueunique)):\n",
        "      TruePos = np.sum((y_true ==y_trueunique[i] ) & (y_pred == y_trueunique[i]))\n",
        "      FalsePos = np.sum((y_true != y_trueunique[i]) & (y_pred == y_trueunique[i]))\n",
        "      FalseNeg = np.sum((y_true == y_trueunique[i]) & (y_pred != y_trueunique[i]))\n",
        "      TrueNeg = np.sum((y_true != y_trueunique[i]) & (y_pred != y_trueunique[i]))\n",
        "      Precisionperclass= TruePos / (TruePos + FalsePos)\n",
        "      Recallperclass= TruePos / (TruePos + FalseNeg)\n",
        "      f1_scoreperclass= 2/((1/Precisionperclass)+(1/Recallperclass))\n",
        "      # MissRateperclass= FalseNeg / (FalseNeg + TruePos)\n",
        "      Precision+= len(y_true[y_true==y_trueunique[i]])*Precisionperclass\n",
        "      Recall+= len(y_true[y_true==y_trueunique[i]])*Recallperclass\n",
        "      f1_score+= len(y_true[y_true==y_trueunique[i]])*f1_scoreperclass\n",
        "      # MissRate+= len(y_true[y_true==y_trueunique[i]])*MissRateperclass\n",
        "    Accuracy= np.mean(y_true == y_pred)\n",
        "    Precision= Precision/len(y_true)\n",
        "    Recall= Recall/len(y_true)\n",
        "    f1_score= f1_score/len(y_true)\n",
        "    # MissRate= MissRate/len(y_true)\n",
        "  return Accuracy, Precision, Recall, f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ghRyGHp798NE"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy, Precision, Recall, f1_score= evaluate_model(y_test, SVM_pred)\n",
        "Accuracy1, Precision1, Recall1, f1_score1= evaluate_model(y_test, LogReg_pred)\n",
        "print(f'SVM Accuracy: {Accuracy}, Precision: {Precision}, Recall: {Recall}, f1_score: {f1_score}')\n",
        "print(f'Logistic regression Accuracy: {Accuracy1}, Precision: {Precision1}, Recall: {Recall1}, f1_score: {f1_score1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtqR7WFlAr71",
        "outputId": "1942acc7-49c1-4547-f563-6fd0d599a42e"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9736842105263158, Precision: 0.9743589743589743, Recall: 0.987012987012987, f1_score: 0.9806451612903224\n",
            "Logistic regression Accuracy: 0.9824561403508771, Precision: 0.9746835443037974, Recall: 1.0, f1_score: 0.9871794871794871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see here, accuracy, precision, recall and f1_score are really high in both models which means that both models exceled to a great extent at identifying the correct classes for all instances (accuracy), out of all the predicted positive instances, a huge portion of them were actually positive (precision) and out of the actual positive instances, a huge portion of them was correctly identified and classified(recall). The balance between recall and precision (f1_score) shows that both models are achieving good score by performing well at both.\n",
        "\n",
        "If we compare between the two models, we can see that they are very similar in performance with almost identical results."
      ],
      "metadata": {
        "id": "NRY_4mQU57db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FalseNeg = np.sum((y_test == 1) & (SVM_pred == 0))\n",
        "TruePos = np.sum((y_test == 1) & (SVM_pred == 1))\n",
        "MissRateSVM= FalseNeg / (FalseNeg + TruePos)\n",
        "print(f'SVM Miss Rate: {MissRateSVM}')\n",
        "FalseNeg = np.sum((y_test == 1) & (LogReg_pred == 0))\n",
        "TruePos = np.sum((y_test == 1) & (LogReg_pred == 1))\n",
        "MissRateLogReg= FalseNeg / (FalseNeg + TruePos)\n",
        "print(f'Logistic regression Miss Rate: {MissRateLogReg}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP3Tkzql0nFR",
        "outputId": "0d018df7-c34a-49d9-cfc1-54737e99cb43"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Miss Rate: 0.012987012987012988\n",
            "Logistic regression Miss Rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to evaluate miss rate for the breast cancer data as the cost of missing a patient is high.\n",
        "Here the cost of a false negative is much higher than a false positive."
      ],
      "metadata": {
        "id": "fPh404ZWNE7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that both models have very low miss rate which is essential when it comes to breast cancer diagnosis. It is almost identifying all breast cancer cases."
      ],
      "metadata": {
        "id": "8oSmFSAb8ViO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfyyoVV5pk8S"
      },
      "source": [
        "## Task 5.1.2 - Multi-Class Classification Evaluation\n",
        "\n",
        "* Do the same as Task 5.1.1 for the multiclass problem `Iris`. Report on the performance metrics: accuracy, precision, recall, F1-score. **(RESULT)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and test the models"
      ],
      "metadata": {
        "id": "--LfzL014_zT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "N1F1_99Hpk8T"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "YULgdTY72fQZ"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "LwougJtW2htf"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM= SVC()\n",
        "SVM.fit(X_train, y_train)\n",
        "SVM_pred = SVM.predict(X_test)"
      ],
      "metadata": {
        "id": "iCc8WBBH2lin"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LogReg= LogisticRegression()\n",
        "LogReg.fit(X_train, y_train)\n",
        "LogReg_pred = LogReg.predict(X_test)"
      ],
      "metadata": {
        "id": "Bu7grGJB2og0"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reporting Performance"
      ],
      "metadata": {
        "id": "Q_yNaOWVN9kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy, Precision, Recall, f1_score= evaluate_model(y_test, SVM_pred)\n",
        "Accuracy1, Precision1, Recall1, f1_score1= evaluate_model(y_test, LogReg_pred)\n",
        "print(f'SVM Accuracy: {Accuracy}, Precision: {Precision}, Recall: {Recall}, f1_score: {f1_score}')\n",
        "print(f'Logistic regression Accuracy: {Accuracy1}, Precision: {Precision1}, Recall: {Recall1}, f1_score: {f1_score1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAO_FVs2A3EQ",
        "outputId": "cbeeaa66-1773-4f12-a1bf-1fb055aea009"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9666666666666667, Precision: 0.9700000000000001, Recall: 0.9666666666666667, f1_score: 0.9668192219679634\n",
            "Logistic regression Accuracy: 0.9333333333333333, Precision: 0.9454545454545454, Recall: 0.9333333333333333, f1_score: 0.9336363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as the binary classification, we can see that both models excel at multi class classification where they nearly correctly classified all instances, had the correctly predicted positives to actual positives ratio as well as the correctly predicted positives to all the predicted positives ratio very high indicating high performance.\n",
        "Also, both models have very comparable and nearly similar results where no model significantly outperformed the other.  "
      ],
      "metadata": {
        "id": "oG8LXI5N8-Im"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgeZH9xhpk8U"
      },
      "source": [
        "## Task 5.1.3 - Regression Evaluation\n",
        "\n",
        "* Now evaluate a trained `Linear Regression` and `SVM` model for the Regression task `Diabetes`. Report on the performance metrics: MSE, RMSE, MAE, RÂ². **(RESULT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and test the models"
      ],
      "metadata": {
        "id": "jBfVAIZu5L7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n"
      ],
      "metadata": {
        "id": "avFmZoHUWBqY"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)"
      ],
      "metadata": {
        "id": "DZz96sNhXGlm"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler= StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YikHZhkGY9my"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "-Y2piIllpk8V"
      },
      "outputs": [],
      "source": [
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "linreg_pred = linreg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_r = SVR()\n",
        "SVM_r.fit(X_train, y_train)\n",
        "SVM_r_pred = SVM_r.predict(X_test)\n"
      ],
      "metadata": {
        "id": "UQa17BASXSRx"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reporting Performance"
      ],
      "metadata": {
        "id": "JXUwnWngOFPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_performance(y_true, y_pred):\n",
        "  MSE = np.sum((y_true - y_pred)**2) / len(y_true)\n",
        "  RMSE = np.sqrt(MSE)\n",
        "  MAE = np.sum(np.abs(y_true - y_pred)) / len(y_true)\n",
        "  sum_squared_error = np.sum((y_true - y_pred)**2)\n",
        "  sum_squared_mean = np.sum((y_true - np.mean(y_true))**2)\n",
        "  R2 = 1 - (sum_squared_error / sum_squared_mean)\n",
        "  return MSE, RMSE, MAE, R2\n"
      ],
      "metadata": {
        "id": "RYnOM051WIuV"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE, RMSE, MAE, R2= report_performance(y_test, linreg_pred)\n",
        "MSE1, RMSE1, MAE1, R21= report_performance(y_test, SVM_r_pred)\n",
        "print(f'Linear Regression MSE: {MSE}, RMSE: {RMSE}, MAE: {MAE}, R2: {R2}')\n",
        "print(f'SVM MSE: {MSE1}, RMSE: {RMSE1}, MAE: {MAE1}, R2: {R21}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95gVZl0tYjLu",
        "outputId": "ea9ecbb3-eee1-4547-a07a-04848dd8425d"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MSE: 3478.1145218115644, RMSE: 58.97554172546077, MAE: 47.8820656544922, R2: 0.43171608364820013\n",
            "SVM MSE: 5091.107121010252, RMSE: 71.3519945131897, MAE: 59.49157478394642, R2: 0.16817164151704467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the linear regression outperforms the SVM which is indicated by the higher level of prediction error of the SVM. Also, it can be seen that the proportion of variance for the diabetes progression(target) that can be explained by the independent features is higher when using linear regression as our model. This is obvious because of the higher r-squared score of the linear regression."
      ],
      "metadata": {
        "id": "HQ55ExOR_MDP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeWroV3gpk8V"
      },
      "source": [
        "## Task 5.1.4 - Cross-Validation (BONUS)\n",
        "\n",
        "* Set up a cross-validation pipeline for the `Linear Regression` and `SVM` models on the `Diabetes` dataset. (Regression) **(RESULT)**\n",
        "* Set up a cross-validation pipeline for the `Logistic Regression` and `SVM` models on the `Iris` dataset. (Classification) **(RESULT)**\n",
        "* Report the performance metrics on all folds (minimum 5-fold) for each model and dataset. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "eEEGrxMZpk8W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PGjI95bepk8X"
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRR5Aztqpk8X"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyforecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}