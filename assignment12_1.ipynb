{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fLCd8b8f2sTs"
      },
      "source": [
        "# Assignment 12.1 - Recurrent Neural Networks\n",
        "\n",
        "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf.\n",
        "\n",
        "#### Please state both names of your group members here:\n",
        "Farah Ahmed Atef Abdelhameed Hafez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "StG9WeHF2sTx"
      },
      "source": [
        "## Task 12.1.1: RNN - 'ShakesGen'\n",
        "\n",
        "Let's create a `ShakesGen` !!<br><br>\n",
        "The data folder contains a shakespeare folder with works from William Shakespeare. Your task is to implement an RNN that learns to write Shakespeare-style text.\n",
        "\n",
        "Below, you'll find all the utility code needed for this task. The Corpus class serves as a dataset, and you can retrieve a batch with its target by calling `get_batch` on a batchified dataset.\n",
        "\n",
        "* Build the missing model components and train your ShakesGen model. **(RESULT)**\n",
        "* Generate at least 30 lines of text using your ShakesGen model. **(RESULT)**\n",
        "\n",
        "Especially, if you train on cpu, you can stop training after 5 minutes and generate based on the current model state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "6E9iNXcA2sTz",
        "outputId": "bc8961f5-994d-49bd-9442-2ab4a8ce5c51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/4000/0*WdbXF_e8kZI1R5nQ.png\" width=\"700\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url= \"https://miro.medium.com/max/4000/0*WdbXF_e8kZI1R5nQ.png\", width=700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cFt8jiSd2sT1"
      },
      "outputs": [],
      "source": [
        "# Some imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.cuda as cuda\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T0j-BXqh2sT3"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "        # This is very english language specific\n",
        "        # We will ingest only these characters:\n",
        "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
        "\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data\n",
        "\n",
        "def get_batch(source, i, bptt_size=35):\n",
        "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get shakespeare folder"
      ],
      "metadata": {
        "id": "VKeo82ZWsvjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BioroboticsLab/ml2526_assignments.git\n",
        "!mkdir -p data\n",
        "!mv ml2526_assignments/data/shakespeare ./data/\n",
        "!rm -rf ml2526_assignments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_J8JeH59hEL",
        "outputId": "581e81c4-5fe2-4597-f455-70497ddc4fe9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ml2526_assignments'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 84 (delta 38), reused 43 (delta 30), pack-reused 30 (from 1)\u001b[K\n",
            "Receiving objects: 100% (84/84), 59.52 MiB | 32.73 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1RCGw7gL2sT5"
      },
      "outputs": [],
      "source": [
        "# Use Corpus to load data\n",
        "corpus = Corpus('./data/shakespeare')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnxy9R-V2sT6",
        "outputId": "3b720659-0333-4f9f-d37d-e4b6746b8d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74010\n",
            "<eos> THE SONNETS <eos> <eos> 1 <eos> <eos> From fairest creatures we desire increase, <eos> That thereby beautys rose might never die, <eos> But as the riper should by time decease, <eos> His tender heir might bear his memory: <eos> But thou contracted to thine own bright eyes, <eos> Feedst thy lights flame with self-substantial fuel, <eos> Making a famine where abundance lies, <eos> Thy self thy foe, to thy sweet self too cruel: <eos> Thou that art now the worlds fresh ornament, <eos> And only herald to the gaudy spring, <eos> Within thine own bud buriest thy content, <eos>\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "print(vocab_size)\n",
        "\n",
        "# Print first 100 words from training data\n",
        "words = [corpus.dictionary.idx2word[corpus.train[i].item()] for i in range(min(100, len(corpus.train)))]\n",
        "print(' '.join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHKl3Z8h2sT8",
        "outputId": "a4879035-7bab-4515-a449-17d0c6bc1745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'That': 10\n"
          ]
        }
      ],
      "source": [
        "idx = corpus.dictionary.word2idx.get(\"That\", -1)  # returns -1 if not found\n",
        "print(f\"Index of the word 'That': {idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2Lj0Vwd62sT9"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN Model"
      ],
      "metadata": {
        "id": "hJhGXwJ0sQyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6vgO2LaQ2sT_"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement RNN model class & Training loop here\n",
        "\n",
        "# Tip: Use an Embedding layer to Tokenize each word.\n",
        "# e.g., self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
        "    super(RNNModel, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.rnn=nn.RNN(embed_dim, hidden_dim, num_layers)\n",
        "    self.fc=nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.embedding(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train loop"
      ],
      "metadata": {
        "id": "9oIns_D_sShB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, batchifieddatasettrain, batchifieddatasetval,optimizer, criterion, epoch, device, bptt=35):\n",
        "    model.train()\n",
        "    for i in range(epoch):\n",
        "      totaltrainloss=0\n",
        "      totalvalloss=0\n",
        "      model.train()\n",
        "      for l in range(0, batchifieddatasettrain.size(0) - 1, bptt):\n",
        "            batch_x, batch_y = get_batch(batchifieddatasettrain, l, bptt)\n",
        "\n",
        "\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            loss = criterion(output, batch_y)\n",
        "            totaltrainloss+=loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "      print(\"Epoch: \", i, \"Average Training Loss: \", totaltrainloss/(batchifieddatasettrain.size(0)// bptt))\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for l in range(0, batchifieddatasetval.size(0) - 1, bptt):\n",
        "          valbatch_x, valbatch_y = get_batch(batchifieddatasetval, l, bptt)\n",
        "          valbatch_x, valbatch_y = valbatch_x.to(device), valbatch_y.to(device)\n",
        "\n",
        "          valoutput=model(valbatch_x)\n",
        "          valoutput = valoutput.view(-1, valoutput.size(-1))\n",
        "          valloss = criterion(valoutput, valbatch_y)\n",
        "          totalvalloss+=valloss.item()\n",
        "\n",
        "      print(\"Epoch: \", i,  \"Average Validation Loss: \", totalvalloss/ (batchifieddatasetval.size(0)//bptt))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bGeWTOWaDBau"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Text Function"
      ],
      "metadata": {
        "id": "xC1MAmdYsV3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateText(model, start_word, corpus, device, mylength=300):\n",
        "    myindex = corpus.dictionary.word2idx[start_word]\n",
        "    input = torch.tensor([[myindex]], device=device)\n",
        "    mywords = [start_word]\n",
        "    model.eval()\n",
        "    for _ in range(mylength):\n",
        "      with torch.no_grad():\n",
        "        logits= model(input)\n",
        "        probs = F.softmax(logits[:, -1], dim=-1)\n",
        "        myindex = torch.multinomial(probs, num_samples=1).item()\n",
        "        word=corpus.dictionary.idx2word[myindex]\n",
        "        input = torch.tensor([[myindex]], device=device)\n",
        "        mywords.append(word)\n",
        "        if mywords[-1]=='<eos>':\n",
        "          break\n",
        "    return ' '.join(mywords)"
      ],
      "metadata": {
        "id": "UlHK8sUVYnVu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Generate"
      ],
      "metadata": {
        "id": "u_8hwC4NsYP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=RNNModel(vocab_size, 50, 128, 1)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batchifieddataset = batchify(corpus.train, 35)\n",
        "batchifieddatasetval = batchify(corpus.valid, 35)\n",
        "train(model, batchifieddataset,batchifieddatasetval, optimizer, criterion, 20, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7jhmzXMEM16",
        "outputId": "f7678606-7a3d-42c4-cacc-8b073f45b258"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Average Training Loss:  7.3161020948077145\n",
            "Epoch:  0 Average Validation Loss:  7.503486352808335\n",
            "Epoch:  1 Average Training Loss:  6.820293290997451\n",
            "Epoch:  1 Average Validation Loss:  7.23349432851754\n",
            "Epoch:  2 Average Training Loss:  6.509971199170598\n",
            "Epoch:  2 Average Validation Loss:  7.005438626981249\n",
            "Epoch:  3 Average Training Loss:  6.275552005138037\n",
            "Epoch:  3 Average Validation Loss:  6.867374261220296\n",
            "Epoch:  4 Average Training Loss:  6.12325286977696\n",
            "Epoch:  4 Average Validation Loss:  6.783494799744849\n",
            "Epoch:  5 Average Training Loss:  6.030118784252203\n",
            "Epoch:  5 Average Validation Loss:  6.735578153647628\n",
            "Epoch:  6 Average Training Loss:  5.968577216818647\n",
            "Epoch:  6 Average Validation Loss:  6.702457745869954\n",
            "Epoch:  7 Average Training Loss:  5.9237416639642895\n",
            "Epoch:  7 Average Validation Loss:  6.68090472501867\n",
            "Epoch:  8 Average Training Loss:  5.889925718869803\n",
            "Epoch:  8 Average Validation Loss:  6.6657591988058655\n",
            "Epoch:  9 Average Training Loss:  5.86139270213415\n",
            "Epoch:  9 Average Validation Loss:  6.657309588264017\n",
            "Epoch:  10 Average Training Loss:  5.837938698957551\n",
            "Epoch:  10 Average Validation Loss:  6.646951432321586\n",
            "Epoch:  11 Average Training Loss:  5.8178582062136455\n",
            "Epoch:  11 Average Validation Loss:  6.643342046176686\n",
            "Epoch:  12 Average Training Loss:  5.8008872315568745\n",
            "Epoch:  12 Average Validation Loss:  6.632939506979549\n",
            "Epoch:  13 Average Training Loss:  5.785362375794716\n",
            "Epoch:  13 Average Validation Loss:  6.631108695385503\n",
            "Epoch:  14 Average Training Loss:  5.7711240767308\n",
            "Epoch:  14 Average Validation Loss:  6.621484214184331\n",
            "Epoch:  15 Average Training Loss:  5.758755117092493\n",
            "Epoch:  15 Average Validation Loss:  6.618623312781839\n",
            "Epoch:  16 Average Training Loss:  5.7485367217153875\n",
            "Epoch:  16 Average Validation Loss:  6.613567576688879\n",
            "Epoch:  17 Average Training Loss:  5.738180659851938\n",
            "Epoch:  17 Average Validation Loss:  6.612206786286597\n",
            "Epoch:  18 Average Training Loss:  5.728123073307973\n",
            "Epoch:  18 Average Validation Loss:  6.618820498971378\n",
            "Epoch:  19 Average Training Loss:  5.718242504686679\n",
            "Epoch:  19 Average Validation Loss:  6.614303224227008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "  print(generateText(model, \"The\", corpus, device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3K7KlNqIySW",
        "outputId": "4ec26dbb-1a50-433c-ed06-0d9fe23c460d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The heart, <eos>\n",
            "The prince's sorrow how we have misuse me oft? <eos>\n",
            "The Duke of Somerset's how? <eos>\n",
            "The King him that neglected beauty be come in us that Called when they [Thunder] <eos>\n",
            "The law to take no man and William Paris; <eos>\n",
            "The heart now <eos>\n",
            "The juggled is from their absent telling peace! And bid it know; my week? <eos>\n",
            "The dog or in some note <eos>\n",
            "The Glendower? of keeping of living. <eos>\n",
            "The 'solus' and I, rob mad prisoners a man, for our praises! in cloak-bag- but anatomy, <eos>\n",
            "The intelligence <eos>\n",
            "The breath but that is ready on my land, what the sound. My blood to repent my Westminster drink the princes runs, being Health to wilfully Lucius <eos>\n",
            "The foolish? <eos>\n",
            "The frailty, <eos>\n",
            "The diadem at Eastcheap on your dare, men of him! Bless him what is hither; foes! <eos>\n",
            "The maids in daggers hook are to to Caesar- <eos>\n",
            "The law? <eos>\n",
            "The hurly wild-cat; <eos>\n",
            "The humour horses-a do not come. <eos>\n",
            "The hour ourselves <eos>\n",
            "The northern parle. <eos>\n",
            "The jest hath you exceed unhappy. <eos>\n",
            "The cut as Friar patient: <eos>\n",
            "The King. <eos>\n",
            "The bawd. I know that he is almost done an diver for no less by several eye, it not grubb'd palpable. <eos>\n",
            "The captain of th' less than you have grieves rely. <eos>\n",
            "The kindness! <eos>\n",
            "The worst in us may it is strange Mardian! <eos>\n",
            "The perpendicular, <eos>\n",
            "The wanton. That I do entertainment; how a lover's, MARCH, <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "  startword=np.random.choice(corpus.dictionary.idx2word)\n",
        "  print(generateText(model, startword , corpus, device))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwpIspcCCzyV",
        "outputId": "bcb4bb66-b6b1-496d-8171-47086d9b08cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "height to his winter bearing a thousand Good cousin, she shall we were temple, a fortunes, hither, <eos>\n",
            "draught, and ready to the other fashion and most hands I be? <eos>\n",
            "stay'd! <eos>\n",
            "Coeur-de-lion proceedings. <eos>\n",
            "biddings to turn both dogs are indeed! I infinite. <eos>\n",
            "Recall <eos>\n",
            "dwelt the rite? <eos>\n",
            "engraft Between the King's select my villaine. <eos>\n",
            "unhorse [Stands stealing, store, <eos>\n",
            "Pomfret, you can my poor Anteroom with bowels, Caesar day under my Deserved I will I well ingratitude <eos>\n",
            "tar- villagery, <eos>\n",
            "confess- <eos>\n",
            "firmness and Redeemer, <eos>\n",
            "Exces <eos>\n",
            "chase, <eos>\n",
            "shoulders thereby discovers render- here be happened will out of mine arms man. Ay, Thou Helena, round 1605 <eos>\n",
            "blood to atone you are fasting by all too letters mistook <eos>\n",
            "garter, a COUNTESSES, Hercules! <eos>\n",
            "lustre; walks. Thaisa._] <eos>\n",
            "fantastical Did you Iras; <eos>\n",
            "folk, Iaylor.] <eos>\n",
            "roof <eos>\n",
            "intents, I'll play scab! <eos>\n",
            "remembered, <eos>\n",
            "story at one Oracle, <eos>\n",
            "harsh-sounding, <eos>\n",
            "o'er-masterest? <eos>\n",
            "CUPID Please you, my publishd thy geck to the Moor, done one remain! <eos>\n",
            "wilt. <eos>\n",
            "LUCRECE rack'd, <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-kaDV9ks2sUA"
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7AF4SJDu_Hi9"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}