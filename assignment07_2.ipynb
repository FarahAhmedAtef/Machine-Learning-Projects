{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy_8vqWQFfHs"
      },
      "source": [
        "# Assignment 7.2 - XGBoost\n",
        "\n",
        "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
        "Please do **NOT** rename the file!\n",
        "\n",
        "#### State both names of your group members here:\n",
        "Farah Ahmed Atef Abdelhameed Hafez - Mariz Essam Sobhy Ghaly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jpzA_JWFfHv"
      },
      "source": [
        "## Task 7.2.1: XGBoost - Regression\n",
        "\n",
        "* Build an XGBoost classifier using `numpy` only. Train your XGBoost model on the `California Housing` regression task. Report on the performance predicting unseen test samples. **(RESULTS)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an XGBoost"
      ],
      "metadata": {
        "id": "RYHJTVzkwzpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ayFVzopZFfHw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Class structure that might help. Feel free to modify as needed.\n",
        "class TreeNode:\n",
        "    \"\"\"Represents a node in the decision tree\"\"\"\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, isleaf=False):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value  # Leaf value\n",
        "        self.is_leaf = isleaf\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"Decision tree for XGBoost\"\"\"\n",
        "    def __init__(self, depth=10, minsamples=3, lambdaterm=1, gamma=0.01):\n",
        "        self.depth = depth\n",
        "        self.minsamples = minsamples\n",
        "        self.root = None\n",
        "        self.lambdaterm=lambdaterm\n",
        "        self.gamma=gamma\n",
        "        # self.task=task\n",
        "\n",
        "    def fit(self, X, gradients, hessians):\n",
        "        \"\"\"Build the tree\"\"\"\n",
        "        self.contordiscrete={}\n",
        "        for i in range(X.shape[1]):\n",
        "            uniqpersample=np.unique(X[:,i])\n",
        "            ratiocontordiscrete=len(uniqpersample)/len(X[:,i])\n",
        "            if ratiocontordiscrete>=0.9:\n",
        "              self.contordiscrete[i]=\"Continous\"\n",
        "            else:\n",
        "              self.contordiscrete[i]=\"Discrete\"\n",
        "        # print(\"mynode\")\n",
        "        self.root = self._build_tree(X,gradients, hessians, 0)\n",
        "\n",
        "\n",
        "    def _build_tree(self, X, gradients, hessians, depth):\n",
        "          \"\"\"Recursively build the tree.\"\"\"\n",
        "          if depth < self.depth and len(X)>self.minsamples:\n",
        "              best_feature, best_threshold, best_gain = self._find_best_split(X, gradients, hessians)\n",
        "\n",
        "\n",
        "              if best_feature is None or best_threshold is None:\n",
        "                  return self.leaf_value(gradients, hessians)\n",
        "              if best_gain<0:\n",
        "                  return self.leaf_value(gradients, hessians)\n",
        "              Xi = X[:, best_feature]\n",
        "              left_mask = Xi <= best_threshold\n",
        "              right_mask = Xi > best_threshold\n",
        "              gradients_left=gradients[left_mask]\n",
        "              hessians_left=hessians[left_mask]\n",
        "              gradients_right=gradients[right_mask]\n",
        "              hessians_right=hessians[right_mask]\n",
        "              X_left=X[left_mask]\n",
        "              X_right=X[right_mask]\n",
        "              depth+=1\n",
        "              left = self._build_tree(X_left,gradients_left, hessians_left, depth)\n",
        "              right = self._build_tree(X_right,gradients_right, hessians_right, depth)\n",
        "              return TreeNode(best_feature,best_threshold,left,right)\n",
        "          return self.leaf_value(gradients, hessians)\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions for X.\"\"\"\n",
        "        y_pred=[]\n",
        "        for x in X:\n",
        "          res=self.root\n",
        "\n",
        "          while res.is_leaf==False:\n",
        "            if x[res.feature_index]<=res.threshold:\n",
        "              res=res.left\n",
        "              # print(\"left\")\n",
        "            else:\n",
        "              res=res.right\n",
        "              # print(\"right\")\n",
        "          y_pred.append(res.value)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "    def _find_best_split(self, X, gradients, hessians):\n",
        "\n",
        "      parentgain= (np.sum(gradients)**2)/(np.sum(hessians)+self.lambdaterm)\n",
        "\n",
        "      featuremsepair={}\n",
        "\n",
        "      for i in range(X.shape[1]):\n",
        "        sorted_idx = np.argsort(X[:, i])\n",
        "        arr_sorted = X[sorted_idx, i]\n",
        "        gradients_sorted=gradients[sorted_idx]\n",
        "        hessians_sorted=hessians[sorted_idx]\n",
        "        sumofgradients=np.cumsum(gradients_sorted) # changed our masking approach done in previous tree impelementation because when it did masking per feature per threshold when there is 20000 plus samples, the computation time is huge\n",
        "        sumofhessians=np.cumsum(hessians_sorted)\n",
        "        scorelist=[]\n",
        "        thresholdlist=[]\n",
        "        if self.contordiscrete[i]==\"Continous\":\n",
        "          lim=X.shape[0]-1\n",
        "        else:\n",
        "          uniquearr_sorted, counts=np.unique(arr_sorted, return_counts=True)\n",
        "          cum_counts = np.cumsum(counts)\n",
        "          lim=len(uniquearr_sorted)\n",
        "        for j in range(lim):\n",
        "\n",
        "          if self.contordiscrete[i]==\"Continous\":\n",
        "            threshold=(arr_sorted[j]+arr_sorted[j+1])/2\n",
        "            k=j\n",
        "          else:\n",
        "            threshold=uniquearr_sorted[j]\n",
        "            k=cum_counts[j]-1\n",
        "\n",
        "          gradients_left=sumofgradients[k]\n",
        "          hessians_left=sumofhessians[k]\n",
        "          gradients_right=sumofgradients[-1]-sumofgradients[k]\n",
        "          hessians_right=sumofhessians[-1]-sumofhessians[k]\n",
        "          if k+1<self.minsamples or len(X)-(k+1)<self.minsamples:\n",
        "              continue\n",
        "\n",
        "          leftgain= (gradients_left**2)/(hessians_left+self.lambdaterm)\n",
        "          rightgain= (gradients_right**2)/(hessians_right+self.lambdaterm)\n",
        "          splitgain=0.5*(leftgain+rightgain-parentgain)-self.gamma\n",
        "          scorelist.append(splitgain)\n",
        "          thresholdlist.append(threshold)\n",
        "        if len(scorelist)==0:\n",
        "          continue\n",
        "        k=np.argmax(np.array(scorelist))\n",
        "        featuremsepair[i]={\"score\":scorelist[k], \"threshold\":thresholdlist[k]}\n",
        "      if len(featuremsepair)==0:\n",
        "        return None, None, None\n",
        "      best_feature = max(featuremsepair, key=lambda i: featuremsepair[i][\"score\"])\n",
        "\n",
        "      bestvalue = featuremsepair[best_feature]\n",
        "\n",
        "      return best_feature, bestvalue[\"threshold\"], bestvalue[\"score\"]\n",
        "\n",
        "\n",
        "    def leaf_value(self, gradients, hessians):\n",
        "      value= -(np.sum(gradients)/(np.sum(hessians)+self.lambdaterm))\n",
        "      return TreeNode(value=value, isleaf=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jEcj_dmeFfHy"
      },
      "outputs": [],
      "source": [
        "class XGBoost:\n",
        "    \"\"\"XGBoost implementation\"\"\"\n",
        "\n",
        "    def __init__(self, boosting_iterations=10, learning_rate=0.1, depth=10, minsamples=3, lambdaterm=1, gamma=0.01):\n",
        "        self.boosting_iterations = boosting_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.trees = []\n",
        "        self.depth=depth\n",
        "        self.minsamples=minsamples\n",
        "        self.lambdaterm=lambdaterm\n",
        "        self.gamma=gamma\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the XGBoost model\"\"\"\n",
        "\n",
        "        uniqpersample=np.unique(y)\n",
        "\n",
        "        if len(uniqpersample)==2:\n",
        "          self.task=\"Classification\"\n",
        "        else:\n",
        "          self.task=\"Regression\"\n",
        "\n",
        "        if self.task==\"Regression\":\n",
        "          prevpreds=np.full_like(y, np.mean(y), dtype=float)\n",
        "          self.firstpred=np.mean(y)\n",
        "          hessians=np.ones_like(y)\n",
        "        else:\n",
        "          prevpreds=np.full_like(y, 0.5, dtype=float)\n",
        "          prevpreds=np.log(prevpreds/(1-prevpreds))\n",
        "          self.firstpred=np.log(0.5/(1-0.5))\n",
        "          hessians=np.full_like(y, 0.5*(1-0.5), dtype=float)\n",
        "\n",
        "\n",
        "        gradients=prevpreds-y\n",
        "\n",
        "        for i in range(self.boosting_iterations):\n",
        "          # print(i)\n",
        "          mytree=DecisionTree(self.depth, self.minsamples, self.lambdaterm, self.gamma)\n",
        "          mytree.fit(X,gradients,hessians)\n",
        "          y_pred=mytree.predict(X)\n",
        "          newpreds=prevpreds+(self.learning_rate*np.array(y_pred))\n",
        "          prevpreds=newpreds\n",
        "          if self.task==\"Classification\":\n",
        "            newpreds=1/(1+np.exp(-newpreds))\n",
        "            hessians=newpreds*(1-newpreds)\n",
        "          gradients=newpreds-y\n",
        "          self.trees.append(mytree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        if self.task==\"Regression\":\n",
        "          return self.predict_helper(X)\n",
        "        else:\n",
        "          return self.predict_proba(X)\n",
        "\n",
        "    def predict_helper(self, X):\n",
        "      y_pred=[]\n",
        "      y_pred.append(np.full(shape=(X.shape[0],), fill_value=self.firstpred, dtype=float))\n",
        "      for tree in self.trees:\n",
        "        treepred=tree.predict(X)\n",
        "        mypreds=np.array(treepred)*self.learning_rate\n",
        "        y_pred.append(mypreds)\n",
        "      y_pred=np.array(y_pred)\n",
        "      return np.sum(y_pred, axis=0)\n",
        "    # Probabilities for classification :) - The Bonus task\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities for binary classification\"\"\"\n",
        "        sum=self.predict_helper(X)\n",
        "        probof1= 1/(1+np.exp(-sum))\n",
        "        return np.where(probof1 >= 0.5, 1, 0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return sum((y_true - y_pred)**2)/len(y_true)"
      ],
      "metadata": {
        "id": "8OAjgNEBjqSK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test on California housing dataset"
      ],
      "metadata": {
        "id": "cnCys83owfhz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dKEcdJAUFfHy"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Load California Housing data\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "myXGboost= XGBoost(boosting_iterations=20)\n",
        "myXGboost.fit(X_train,y_train)\n",
        "y_pred=myXGboost.predict(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IlTraj0FfHz",
        "outputId": "b0ade7e2-9faa-438c-aa56-dd17ae6a0dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error  0.2871924889390944\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean Squared Error \", mean_squared_error(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtHtsql6FfHz"
      },
      "source": [
        "## Task 7.2.2: XGBoost - Classification (BONUS)\n",
        "\n",
        "* Train an XGBoost model on the `Breast Cancer` binary classification task. Report on the performance predicting unseen test samples. **(RESULTS)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on the breast cancer dataset"
      ],
      "metadata": {
        "id": "3BZXkXL-wkrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5-kguovZFfH0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Access the features and labels\n",
        "X = data.data  # Shape: (569, 30)\n",
        "y = data.target  # Shape: (569,) - 0 for malignant, 1 for benign\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "myXGboost= XGBoost(boosting_iterations=20)\n",
        "myXGboost.fit(X_train,y_train)\n",
        "y_pred=myXGboost.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzs8gW_VFfH1",
        "outputId": "6c8833e8-32c8-4391-9878-227bdce7d7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy  0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy \", np.mean(y_test==y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0ugv72FfH1"
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QNWjOIqFfH1"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyforecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}