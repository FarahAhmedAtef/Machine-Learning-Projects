{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfPDVQ9jnPw9"
      },
      "source": [
        "---\n",
        "\n",
        "## Grading Info/Details - Assignment 4.1:\n",
        "\n",
        "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
        "\n",
        "* For passing the test scripts:\n",
        "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
        "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
        "\n",
        "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
        "\n",
        "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o5qE0EvPnPw-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9unfJknPw_"
      },
      "source": [
        "## Task 4.1.1 - PCA from Scratch\n",
        "\n",
        "Implement Principal Component Analysis (PCA) from scratch using only `NumPy`.\n",
        "This assignment will help you understand the mathematical foundations of PCA.\n",
        "\n",
        "* Implement the PCA given the class structure below. **(RESULT)**\n",
        "* Test your implementation using small synthetic datasets described in the test functions below. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kmHv7UdlnPxA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAeT4qpx44fx"
      },
      "source": [
        "# Build PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "80PXIk0BnPxB"
      },
      "outputs": [],
      "source": [
        "class PCA:\n",
        "    \"\"\"\n",
        "    Principal Component Analysis implementation using only NumPy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_components=2):\n",
        "        \"\"\"\n",
        "        Initialize PCA.\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit PCA on the training data X.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this function\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        X = X - self.mean_\n",
        "\n",
        "        cov = np.dot(X.transpose(), X)\n",
        "        S, W = np.linalg.eig(cov)\n",
        "        idx = np.argsort(S)[::-1]\n",
        "        self.eigenvectors = W[:, idx]\n",
        "        self.explained_variance_ = S[idx][:self.n_components]\n",
        "        self.components= self.eigenvectors[:, :self.n_components]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self, X, dim=5):\n",
        "        \"\"\"\n",
        "        Transform X into the principal component space.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this function\n",
        "        X = X - self.mean_\n",
        "        return np.dot(X, self.components)\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform data back to original space.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this function\n",
        "        return X.dot(self.components.transpose()) + self.mean_\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Fit PCA and transform X in one step.\n",
        "        \"\"\"\n",
        "        self.fit(X)\n",
        "        return self.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThXO6cne9oz_"
      },
      "source": [
        "# Testing decorrelation, dimensionality reduction and reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aZ8isSl8nPxC"
      },
      "outputs": [],
      "source": [
        "def test_basic_pca():\n",
        "    \"\"\"Test 1: Basic PCA on 2D data\"\"\"\n",
        "    # TODO: Implement this function\n",
        "    #decorrelation\n",
        "    np.random.seed(42)\n",
        "\n",
        "\n",
        "    data_2d = np.random.rand(100, 2)\n",
        "    # print(data_2d.shape)\n",
        "    pca = PCA(n_components=2)\n",
        "    transformeddata = pca.fit_transform(data_2d)\n",
        "    # print(transformeddata.shape)\n",
        "    return  transformeddata, data_2d, pca\n",
        "\n",
        "\n",
        "def test_dimensionality_reduction():\n",
        "    \"\"\"Test 2: Reduce 5D data to 2D\"\"\"\n",
        "    # TODO: Implement this function\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Create a dataset with 100 samples and 5 features (5D)\n",
        "    data_5d = np.random.rand(100, 5)\n",
        "    # print(data_5d.shape)\n",
        "    pca = PCA(n_components=2)\n",
        "    data_2d = pca.fit_transform(data_5d)\n",
        "    # print(data_2d.shape)\n",
        "    return data_2d, data_5d, pca\n",
        "\n",
        "\n",
        "def test_reconstruction():\n",
        "    \"\"\"Test 3: Inverse transform (reconstruction)\"\"\"\n",
        "    # TODO: Implement this function\n",
        "    reduced, original, pca=test_dimensionality_reduction()\n",
        "    reconstructed = pca.inverse_transform(reduced)\n",
        "    print( \"Reconstruction error:\", np.linalg.norm(original - reconstructed, ord='fro'))\n",
        "\n",
        "\n",
        "\n",
        "def test_variance_ordering():\n",
        "    \"\"\"Test 4: Components are ordered by variance\"\"\"\n",
        "    print(\"Test 4: Verify components are ordered by explained variance\")\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(100, 5)\n",
        "\n",
        "    pca = PCA(n_components=5)\n",
        "    pca.fit(X)\n",
        "\n",
        "    # Check that explained variances are in descending order\n",
        "    variances = pca.explained_variance_\n",
        "    is_sorted = np.all(variances[:-1] >= variances[1:])\n",
        "\n",
        "    print(f\"Explained variances: {variances}\")\n",
        "    print(f\"Is sorted (descending): {is_sorted}\")\n",
        "    assert is_sorted, \"Components not sorted by variance!\"\n",
        "    print(\"✓ Test 4 passed\\n\")\n",
        "\n",
        "\n",
        "def test_centered_data():\n",
        "    \"\"\"Test 5: Verify data is properly centered\"\"\"\n",
        "    print(\"Test 5: Verify data centering\")\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(100, 3) + 10  # Add offset\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    pca.fit(X)\n",
        "\n",
        "    # Mean should be close to the original data mean\n",
        "    print(f\"Original data mean: {np.mean(X, axis=0)}\")\n",
        "    print(f\"Stored mean: {pca.mean_}\")\n",
        "    print(f\"Difference: {np.mean(np.abs(np.mean(X, axis=0) - pca.mean_)):.10f}\")\n",
        "    print(\"✓ Test 5 passed\\n\")\n",
        "\n",
        "\n",
        "def run_all_tests():\n",
        "    print(\"Running PCA test suite...\\n\")\n",
        "    try:\n",
        "      test_basic_pca()\n",
        "      test_dimensionality_reduction()\n",
        "      test_reconstruction()\n",
        "      test_variance_ordering()\n",
        "      test_centered_data()\n",
        "\n",
        "      print(\"ALL TESTS PASSED!\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"\\n❌ Test failed: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Unexpected error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcIRgMTgZVFs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfx6tPU_nPxD",
        "outputId": "cc91de52-9c9a-46b7-e15f-ff46300ecb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running PCA test suite...\n",
            "\n",
            "Reconstruction error: 4.581361017952531\n",
            "Test 4: Verify components are ordered by explained variance\n",
            "Explained variances: [125.2632111  102.95072664  96.46122129  86.48035729  65.91672829]\n",
            "Is sorted (descending): True\n",
            "✓ Test 4 passed\n",
            "\n",
            "Test 5: Verify data centering\n",
            "Original data mean: [10.09176598  9.81676669 10.07482166]\n",
            "Stored mean: [10.09176598  9.81676669 10.07482166]\n",
            "Difference: 0.0000000000\n",
            "✓ Test 5 passed\n",
            "\n",
            "ALL TESTS PASSED!\n"
          ]
        }
      ],
      "source": [
        "# Run the test suite\n",
        "run_all_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6xJjgJ9Cnh2"
      },
      "source": [
        "As you can see, the mean is matched and the variance is ordered correctly. Also, the dimensions were reduced from 5 to 2 (checked by the commented print statement). Also, the reconstruction error is relatively small which means that reconstruction was generally successful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xkPu-opnPxE"
      },
      "source": [
        "## Task 4.1.2 - PCA on Real-World Data\n",
        "\n",
        "* Apply your PCA implementation on the `California Housing Dataset`. **(RESULT)**\n",
        "* Compare your results with those obtained from the scikit-learn PCA implementation: `sklearn.decomposition.PCA`. Are your results within numerical precision? **(RESULT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2stJeOjanPxF"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.decomposition import PCA as SklearnPCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hgfSqLS8LiY"
      },
      "source": [
        "#Testing california housing dataset using Scikit-learn PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz9OvmKknPxF",
        "outputId": "26b05a4e-3e9d-43bb-f5a0-02f72f889ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reconstruction error (Scikit-learn implementation): 259.1595704420067\n"
          ]
        }
      ],
      "source": [
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "\n",
        "pca_2 = SklearnPCA(n_components=5)\n",
        "X_pca = pca_2.fit_transform(X)\n",
        "X_recon = pca_2.inverse_transform(X_pca)\n",
        "recon_error = np.linalg.norm(X - X_recon, ord='fro')\n",
        "print(\"Reconstruction error (Scikit-learn implementation):\", recon_error)\n",
        "\n",
        "var_sklearn =  X_pca.var(axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZI5_Dpz9CLv"
      },
      "source": [
        "# Testing california housing dataset using our custom made PCA class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYBcUoWxLBiU",
        "outputId": "82d8c675-8ea8-469f-c348-4f6662b6af17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reconstruction error (Custom implementation): 259.1595704420065\n",
            "Difference between Scikit-learn reduced dataset variance and custome PCA reduced dataset variance 2.328313557364388e-10\n",
            "Difference between Scikit-learn reconstruction error and custome PCA reconstruction error 2.2737367544323206e-13\n"
          ]
        }
      ],
      "source": [
        "pca2 = PCA(n_components=5)\n",
        "reduced = pca2.fit_transform(X)\n",
        "# print(y.shape)\n",
        "reconstructed = pca2.inverse_transform(reduced)\n",
        "error = np.linalg.norm(X - reconstructed, ord='fro')\n",
        "print(\"Reconstruction error (Custom implementation):\", error)\n",
        "var_red =  reduced.var(axis=0)\n",
        "\n",
        "diff = np.linalg.norm(var_sklearn - var_red)\n",
        "print(\"Difference between Scikit-learn reduced dataset variance and custome PCA reduced dataset variance\",diff)\n",
        "print(\"Difference between Scikit-learn reconstruction error and custome PCA reconstruction error\",recon_error-error)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoAPaiR_-_Im"
      },
      "source": [
        "The results of our implementation is very similar to scikit-learn's implementation. This is shown by comparing the reconstruction error of both implementations which is almost (within numeric precision) the same for both.  We also compared the variance of the reduced dataset in both implementations where the difference between them showed that they are within numeric precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vMZCmqkPnPxG",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZboWjbInPxG"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pyforecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
