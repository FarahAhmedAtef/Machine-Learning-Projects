{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8gN1gsCaQDi"
      },
      "source": [
        "# Assignment 6.2 - More Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF9L9UzcaQDj"
      },
      "source": [
        "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
        "Please do **NOT** rename the file!\n",
        "\n",
        "#### State both names of your group members here:\n",
        "[Jane and John Doe]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 557,
      "metadata": {
        "id": "vAnBwtvzaQDk"
      },
      "outputs": [],
      "source": [
        "# Farah Ahmed Atef Abdelhameed Hafez- Mariz Essam Sobhy Ghaly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbxhAkGOaQDl"
      },
      "source": [
        "---\n",
        "\n",
        "## Grading Info/Details - Assignment 6.2:\n",
        "\n",
        "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
        "\n",
        "* For passing the test scripts:\n",
        "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
        "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
        "\n",
        "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
        "\n",
        "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 557,
      "metadata": {
        "id": "mf47ezIYaQDl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWm6sbogaQDm"
      },
      "source": [
        "## Task 6.2.1 - Classification Trees\n",
        "\n",
        "* Implement the Classification Tree Class from scratch using only `NumPy`. The splitting criterion should be Gini impurity. **(RESULT)**\n",
        "* Run your implementation on the `IRIS` classification dataset. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 558,
      "metadata": {
        "id": "3IXos8ObaQDm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature, threshold, left, right):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "\n",
        "class DecisionTree(ABC):\n",
        "    \"\"\"Base class for a decision tree.\"\"\"\n",
        "    def __init__(self,depth=6, random_features=False, minsamples=3):\n",
        "        self.depth = depth\n",
        "        self.Random_features = random_features\n",
        "        self.minsamples=minsamples\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build the regression tree.\"\"\"\n",
        "        self.X=X\n",
        "        self.y=y\n",
        "        thresholds=[]\n",
        "        self.contordiscrete={}\n",
        "        for i in range(X.shape[1]):\n",
        "              uniqpersample=np.unique(X[:,i])\n",
        "              ratiocontordiscrete=len(uniqpersample)/len(X[:,i])\n",
        "              if ratiocontordiscrete>=0.9:\n",
        "                self.contordiscrete[i]=\"Continous\"\n",
        "              else:\n",
        "                self.contordiscrete[i]=\"Discrete\"\n",
        "        self.rootnode = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X,  y, depth):\n",
        "        \"\"\"Recursively build the tree.\"\"\"\n",
        "        if depth < self.depth and len(X)>self.minsamples:\n",
        "            # print(X.shape[0])\n",
        "\n",
        "            X_indices=np.arange(X.shape[1])\n",
        "\n",
        "\n",
        "            Xrandom=X\n",
        "            if self.Random_features:\n",
        "              X_indices = np.random.choice(X.shape[1], size=int(np.sqrt(X.shape[1])), replace=False)\n",
        "              Xrandom= X[:, X_indices]\n",
        "\n",
        "\n",
        "\n",
        "            thresholds=[]\n",
        "            for i in range(Xrandom.shape[1]):\n",
        "              sorted_idx = np.argsort(Xrandom[:, i])\n",
        "              arr_sorted = Xrandom[sorted_idx, i]\n",
        "              if self.contordiscrete[X_indices[i]]==\"Continous\":\n",
        "                thresholds.append([(arr_sorted[j] + arr_sorted[j+1]) / 2 for j in range(len(arr_sorted) - 1)])\n",
        "              else:\n",
        "                thresholds.append([j for j in np.unique(arr_sorted)])\n",
        "\n",
        "            best_feature, best_threshold = self._find_best_split(Xrandom, y, X_indices, thresholds)\n",
        "            if best_feature is None or best_threshold is None:\n",
        "                return self._leaf_value(y)\n",
        "            X_left, y_left, X_right, y_right=self.splitmydata(X,y, best_feature, best_threshold)\n",
        "            depth+=1\n",
        "            # print(\"left\",X.shape[0], X_left.shape[0])\n",
        "            left = self._build_tree(X_left,y_left,depth)\n",
        "            # print(\"right\",X.shape[0], X_right.shape[0])\n",
        "            right = self._build_tree(X_right,y_right, depth)\n",
        "            return TreeNode(best_feature,best_threshold,left,right)\n",
        "        return self._leaf_value(y)\n",
        "\n",
        "    def splitmydata(self, X,y, best_feature, best_threshold):\n",
        "\n",
        "      Xi = X[:, best_feature]\n",
        "      left_mask = Xi <= best_threshold\n",
        "      right_mask = Xi > best_threshold\n",
        "      y_left=y[left_mask]\n",
        "      y_right=y[right_mask]\n",
        "      X_left=X[left_mask]\n",
        "      X_right=X[right_mask]\n",
        "      return X_left, y_left, X_right, y_right\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions for X.\"\"\"\n",
        "        y_pred=[]\n",
        "        for x in X:\n",
        "          res=self.rootnode\n",
        "\n",
        "          while isinstance(res, TreeNode):\n",
        "            if x[res.feature]<=res.threshold:\n",
        "              res=res.left\n",
        "            else:\n",
        "              res=res.right\n",
        "          y_pred.append(res)\n",
        "        return y_pred\n",
        "\n",
        "    def _find_best_split(self, X, y,X_indices, thresholds=None):\n",
        "        \"\"\"Find the best split.\"\"\"\n",
        "\n",
        "        featuremsepair={}\n",
        "        for i in range(X.shape[1]):\n",
        "          scorelist=[]\n",
        "          thresholdlist=[]\n",
        "          for threshold in thresholds[i]:\n",
        "            Xi = X[:, i]\n",
        "            left_mask = Xi <= threshold\n",
        "            right_mask = Xi > threshold\n",
        "            y_left=y[left_mask]\n",
        "            y_right=y[right_mask]\n",
        "            X_left=X[left_mask]\n",
        "            X_right=X[right_mask]\n",
        "            if y_left.size <self.minsamples or y_right.size <self.minsamples:\n",
        "                continue\n",
        "            thresholdscore= self._calculate_featurescore(y_left, y_right, y)\n",
        "\n",
        "            scorelist.append(thresholdscore)\n",
        "            thresholdlist.append(threshold)\n",
        "          if len(scorelist)==0:\n",
        "            continue\n",
        "          bestthresholdforthisfeature= self.find_best_threshold( scorelist)\n",
        "          featuremsepair[X_indices[i]]={\"bestscore\":scorelist[bestthresholdforthisfeature], \"threshold\":thresholdlist[bestthresholdforthisfeature]}\n",
        "        if len(featuremsepair)==0:\n",
        "          return None, None\n",
        "\n",
        "        best_feature= self.find_bestfeature( featuremsepair)\n",
        "\n",
        "        bestvalue = featuremsepair[best_feature]\n",
        "\n",
        "        return best_feature, bestvalue[\"threshold\"]\n",
        "\n",
        "    @abstractmethod\n",
        "    def _leaf_value(self, y):\n",
        "      \"\"\"Compute the leaf value.\"\"\"\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _calculate_featurescore(self, y_left, y_right):\n",
        "      pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def find_bestfeature(self):\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 559,
      "metadata": {
        "id": "mKVLKKiaaQDn"
      },
      "outputs": [],
      "source": [
        "class ClassificationTree(DecisionTree):\n",
        "    \"\"\"Classification decision tree using Gini impurity.\"\"\"\n",
        "    def __init__(self, depth=6, minsamples=3,random_features=False):\n",
        "        super().__init__(depth, random_features, minsamples)\n",
        "\n",
        "\n",
        "    def _leaf_value(self, y):\n",
        "        # print(y)\n",
        "        y_vals, y_counts= np.unique(y, return_counts=True)\n",
        "        # print(y_vals[np.argmax(y_counts)])\n",
        "        return y_vals[np.argmax(y_counts)]\n",
        "\n",
        "    def _calculate_featurescore(self, y_left, y_right, y):\n",
        "\n",
        "      y_vals, y_counts= np.unique(y, return_counts=True)\n",
        "      pmk_y=[]\n",
        "      for i in range(len(y_vals)):\n",
        "          pmk_y.append(y_counts[i]/len(y))\n",
        "      temp_y= [pmk*(1-pmk) for pmk in pmk_y]\n",
        "      gini_index_parent = sum(temp_y)\n",
        "\n",
        "      y_left_values, y_left_count= np.unique(y_left, return_counts=True)\n",
        "      pmk_left=[]\n",
        "      for i in range(len(y_left_values)):\n",
        "        pmk_left.append(y_left_count[i]/len(y_left))\n",
        "\n",
        "      y_right_values, y_right_count= np.unique(y_right, return_counts=True)\n",
        "      pmk_right=[]\n",
        "      for i in range(len(y_right_values)):\n",
        "        pmk_right.append(y_right_count[i]/len(y_right))\n",
        "      temp_left= [pmk*(1-pmk) for pmk in pmk_left]\n",
        "      gini_index_left = sum(temp_left)\n",
        "      temp_right= [pmk*(1-pmk) for pmk in pmk_right]\n",
        "      gini_index_right = sum(temp_right)\n",
        "      weighted_gini_index = (len(y_left)/(len(y_left)+len(y_right)))*gini_index_left + (len(y_right)/(len(y_left)+len(y_right)))*gini_index_right\n",
        "      return gini_index_parent-weighted_gini_index\n",
        "\n",
        "\n",
        "    def find_best_threshold(self, scorelist):\n",
        "      return np.argmax(scorelist)\n",
        "\n",
        "    def find_bestfeature(self, featuremsepair):\n",
        "\n",
        "      return max(featuremsepair, key=lambda i: featuremsepair[i][\"bestscore\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X,y=load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "tree=ClassificationTree()\n",
        "tree.fit(X,y)\n",
        "y_pred=tree.predict(X_test)\n",
        "print(\"Accuracy\", np.mean(y_test==y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VsQ8ceC1F67",
        "outputId": "f9b752dd-63b5-4be1-ab17-e07bc16f191c"
      },
      "execution_count": 560,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_V-ckrKaQDo"
      },
      "source": [
        "## Task 6.2.2 - Random Forests\n",
        "\n",
        "* Implement Random Forests using only `NumPy`. **(RESULT)**\n",
        "* Compare the results between the random forest run of your `ClassificationTree` class on the `IRIS` dataset. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 561,
      "metadata": {
        "id": "id5G990WaQDo"
      },
      "outputs": [],
      "source": [
        "class RandomForest:\n",
        "    \"\"\"Random Forest Classifier.\"\"\"\n",
        "\n",
        "    def __init__(self, bags, n_samples, depth=6, minsamples=3):\n",
        "        self.bags = bags\n",
        "        self.samples = n_samples\n",
        "        self.depth=depth\n",
        "        self.minsamples=minsamples\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      self.dict_bagged_trees={}\n",
        "      for i in range(self.bags):\n",
        "        bag_indices = np.random.choice(len(X), size=self.samples, replace=True)\n",
        "        bag_X = X[bag_indices]\n",
        "        bag_y = y[bag_indices]\n",
        "        tree = ClassificationTree(self.depth,self.minsamples,random_features=True)\n",
        "        tree.fit(bag_X, bag_y)\n",
        "        self.dict_bagged_trees[i]=tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        treepred=[]\n",
        "        finaly_pred=[]\n",
        "        for i in range(self.bags):\n",
        "          tree=self.dict_bagged_trees[i]\n",
        "          y_pred=tree.predict(X)\n",
        "          treepred.append(y_pred)\n",
        "        treepred=np.array(treepred)\n",
        "        for i in range(treepred.shape[1]):\n",
        "          temp= treepred[:,i]\n",
        "          y_vals, y_counts= np.unique(temp, return_counts=True)\n",
        "          finaly_pred.append(y_vals[np.argmax(y_counts)])\n",
        "        return np.array(finaly_pred)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 562,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaGzLQYCaQDo",
        "outputId": "96209c5f-e934-44e0-8045-96c925520a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1.0\n"
          ]
        }
      ],
      "source": [
        "tree=RandomForest(20, 60)\n",
        "tree.fit(X,y)\n",
        "y_pred=tree.predict(X_test)\n",
        "print(\"Accuracy\", np.mean(y_test==y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that both random forest and single tree have very high same results on the iris dataset. This is because it is a toy data set."
      ],
      "metadata": {
        "id": "932EXSyF_iwe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP-aTyi_aQDo"
      },
      "source": [
        "## Task 6.2.3 - Extra Trees\n",
        "\n",
        "* Implement Extra Trees using only `NumPy`. **(RESULT)**\n",
        "* Compare the results between the `Random Forest` and an `Extra Trees` ensemble implementation on the `IRIS` dataset. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 563,
      "metadata": {
        "id": "2QCzPZXIaQDo"
      },
      "outputs": [],
      "source": [
        "class ExtraTree(ClassificationTree):\n",
        "    \"\"\"Extremely Randomized Tree - uses random thresholds instead of optimal ones.\"\"\"\n",
        "\n",
        "    def __init__(self, n_thresholds, depth=6, minsamples=3):\n",
        "        super().__init__(depth,minsamples)\n",
        "        self.n_thresholds = n_thresholds\n",
        "        self.depth=depth\n",
        "        self.minsamples=minsamples\n",
        "        # self.Random_fearure=False\n",
        "\n",
        "    def _build_tree(self, X, y, depth, thresholds=None):\n",
        "        \"\"\"Recursively build the tree.\"\"\"\n",
        "        if depth < self.depth:\n",
        "            thresholds=[]\n",
        "            X_indices = np.random.choice(X.shape[1], size=int(np.sqrt(X.shape[1])), replace=False)\n",
        "            Xrandom=X[:, X_indices]\n",
        "            for i in X_indices:\n",
        "              if self.contordiscrete[i]==\"Continous\":\n",
        "                thresholds.append(np.random.uniform(min(X[:,i]), max(X[:,i]), size=self.n_thresholds))\n",
        "              else:\n",
        "                unique_vals = np.unique(X[:, i])\n",
        "                num_thresholds = min(self.n_thresholds, len(unique_vals))\n",
        "                thresholds.append(np.random.choice(X[:,i], size=num_thresholds, replace=False))\n",
        "            self.Random_fearure=False\n",
        "            best_feature, best_threshold = self._find_best_split(Xrandom, y, X_indices, thresholds)\n",
        "            if best_feature is None or best_threshold is None:\n",
        "                return self._leaf_value(y)\n",
        "            X_left, y_left, X_right, y_right=self.splitmydata( X,y, best_feature, best_threshold)\n",
        "            depth+=1\n",
        "            left = self._build_tree(X_left,y_left,depth)\n",
        "            right = self._build_tree(X_right,y_right, depth)\n",
        "            return TreeNode(best_feature,best_threshold,left,right)\n",
        "        return self._leaf_value(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class ExtraTree(DecisionTree):\n",
        "#     \"\"\"Extremely Randomized Tree - uses random thresholds instead of optimal ones.\"\"\"\n",
        "\n",
        "#     def __init__(self, n_thresholds, depth=6, minsamples=3):\n",
        "#         super().__init__(depth, False,minsamples)\n",
        "#         self.n_thresholds = n_thresholds\n",
        "#         self.depth=depth\n",
        "#         self.minsamples=minsamples\n",
        "#         # self.Random_fearure=False\n",
        "\n",
        "#     def _build_tree(self, X, y, depth, thresholds=None):\n",
        "#         \"\"\"Recursively build the tree.\"\"\"\n",
        "#         if depth < self.depth:\n",
        "#             thresholds=[]\n",
        "#             X_indices = np.random.choice(X.shape[1], size=int(np.sqrt(X.shape[1])), replace=False)\n",
        "#             Xrandom=X[:, X_indices]\n",
        "#             for i in X_indices:\n",
        "#               if self.contordiscrete[i]==\"Continous\":\n",
        "#                 thresholds.append(np.random.uniform(min(X[:,i]), max(X[:,i]), size=self.n_thresholds))\n",
        "#               else:\n",
        "#                 unique_vals = np.unique(X[:, i])\n",
        "#                 num_thresholds = min(self.n_thresholds, len(unique_vals))\n",
        "#                 thresholds.append(np.random.choice(X[:,i], size=num_thresholds, replace=False))\n",
        "#             self.Random_fearure=False\n",
        "#             best_feature, best_threshold = self._find_best_split(Xrandom, y, X_indices, thresholds)\n",
        "#             if best_feature is None or best_threshold is None:\n",
        "#                 return self._leaf_value(y)\n",
        "#             X_left, y_left, X_right, y_right=self.splitmydata( X,y, best_feature, best_threshold)\n",
        "#             depth+=1\n",
        "#             left = self._build_tree(X_left,y_left,depth)\n",
        "#             right = self._build_tree(X_right,y_right, depth)\n",
        "#             return TreeNode(best_feature,best_threshold,left,right)\n",
        "#         return self._leaf_value(y)\n",
        "\n",
        "#     def _leaf_value(self, y):\n",
        "#         # return np.bincount(y).argmax()\n",
        "#         # print(y)\n",
        "#         y_vals, y_counts= np.unique(y, return_counts=True)\n",
        "#         # print(y_vals[np.argmax(y_counts)])\n",
        "#         return y_vals[np.argmax(y_counts)]\n",
        "\n",
        "#     def _calculate_featurescore(self, y_left, y_right,y):\n",
        "#       y_vals, y_counts= np.unique(y, return_counts=True)\n",
        "#       pmk_y=[]\n",
        "#       for i in range(len(y_vals)):\n",
        "#           pmk_y.append(y_counts[i]/len(y))\n",
        "#       temp_y= [pmk*(1-pmk) for pmk in pmk_y]\n",
        "#       gini_index_parent = sum(temp_y)\n",
        "\n",
        "#       y_left_values, y_left_count= np.unique(y_left, return_counts=True)\n",
        "#       pmk_left=[]\n",
        "#       for i in range(len(y_left_values)):\n",
        "#         pmk_left.append(y_left_count[i]/len(y_left))\n",
        "\n",
        "#       y_right_values, y_right_count= np.unique(y_right, return_counts=True)\n",
        "#       pmk_right=[]\n",
        "#       for i in range(len(y_right_values)):\n",
        "#         pmk_right.append(y_right_count[i]/len(y_right))\n",
        "#       temp_left= [pmk*(1-pmk) for pmk in pmk_left]\n",
        "#       gini_index_left = sum(temp_left)\n",
        "#       temp_right= [pmk*(1-pmk) for pmk in pmk_right]\n",
        "#       gini_index_right = sum(temp_right)\n",
        "#       weighted_gini_index = (len(y_left)/(len(y_left)+len(y_right)))*gini_index_left + (len(y_right)/(len(y_left)+len(y_right)))*gini_index_right\n",
        "#       return gini_index_parent-weighted_gini_index\n",
        "\n",
        "\n",
        "#     def find_best_threshold(self, scorelist):\n",
        "#       return np.argmax(scorelist)\n",
        "\n",
        "#     def find_bestfeature(self, featuremsepair):\n",
        "\n",
        "#       return max(featuremsepair, key=lambda i: featuremsepair[i][\"bestscore\"])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CQfrUe0smL3g"
      },
      "execution_count": 564,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We altered the inheritance to inherit from classification tree since it is doing a classification task. But in case, it is not accepted. The original inheritance is found above(Uncomment and test)."
      ],
      "metadata": {
        "id": "TGRZgUzZAOB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 565,
      "metadata": {
        "id": "J84p5J6LaQDp"
      },
      "outputs": [],
      "source": [
        "class ExtraTrees:\n",
        "    \"\"\"Extremely Randomized Trees ensemble.\"\"\"\n",
        "    def __init__(self, n_trees, n_thresholds, n_samples=0, bagging=False, depth=6, minsamples=3):\n",
        "        self.n_trees = n_trees\n",
        "        self.samples = n_samples\n",
        "        self.bagging = bagging\n",
        "        self.n_thresholds = n_thresholds\n",
        "        self.depth=depth\n",
        "        self.minsamples=minsamples\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      self.dict_trees={}\n",
        "      if self.bagging:\n",
        "        for i in range(self.n_trees):\n",
        "          bag_indices = np.random.choice(len(X), size=self.samples, replace=True)\n",
        "          bag_X = X[bag_indices]\n",
        "          bag_y = y[bag_indices]\n",
        "          tree = ExtraTree(self.n_thresholds, self.depth, self.minsamples)\n",
        "          tree.fit(bag_X, bag_y)\n",
        "          self.dict_trees[i]=tree\n",
        "      else:\n",
        "        for i in range(self.n_trees):\n",
        "          tree = ExtraTree(self.n_thresholds, self.depth, self.minsamples)\n",
        "          tree.fit(X, y)\n",
        "          self.dict_trees[i]=tree\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions by averaging all trees.\"\"\"\n",
        "        treepred=[]\n",
        "        finaly_pred=[]\n",
        "        for i in range(self.n_trees):\n",
        "          tree=self.dict_trees[i]\n",
        "          y_pred=tree.predict(X)\n",
        "          # print(y_pred)\n",
        "          treepred.append(y_pred)\n",
        "        treepred=np.array(treepred)\n",
        "\n",
        "        for i in range(treepred.shape[1]):\n",
        "          temp= treepred[:,i]\n",
        "          y_vals, y_counts= np.unique(temp, return_counts=True)\n",
        "          finaly_pred.append(y_vals[np.argmax(y_counts)])\n",
        "        return np.array(finaly_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bagging enabled"
      ],
      "metadata": {
        "id": "43FQ8sIzA-ya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 566,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLTF8HfnaQDp",
        "outputId": "6d0bb256-e5f7-40e1-eca0-17e1adaa80dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1.0\n"
          ]
        }
      ],
      "source": [
        "tree=ExtraTrees(10,30, True)\n",
        "tree.fit(X,y)\n",
        "y_pred=tree.predict(X_test)\n",
        "print(\"Accuracy\", np.mean(y_test==y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bagging disabled"
      ],
      "metadata": {
        "id": "2M5jI8ETBCT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree=ExtraTrees(10,30, False)\n",
        "tree.fit(X,y)\n",
        "y_pred=tree.predict(X_test)\n",
        "print(\"Accuracy\", np.mean(y_test==y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6FFhoUSA64D",
        "outputId": "65cace10-178c-449c-a1a0-e32fec0f0d0e"
      },
      "execution_count": 567,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that whether bagging is enabled or not, it has very high performance the same as random forest. It i expected to be better in generalisation in other problems than random forest which is not aparent here beacuse iris dataset is a toy dataset as mentioned before."
      ],
      "metadata": {
        "id": "rT8LzXdyCWlT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Ddatlr7RaQDp"
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccacllr1aQDp"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyforecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}