{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veVOiCPOih-b"
      },
      "source": [
        "# Assignment 11.1 - Transformer\n",
        "\n",
        "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HKPHYiuih-c"
      },
      "source": [
        "#### Please state both names of your group members here:\n",
        "Farah Ahmed Atef Abdelhameed Hafez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gi0wYD2ih-c"
      },
      "source": [
        "## Task 11.1.1: Self-Attention\n",
        "\n",
        "Implement the attention mechanism by yourself. You are free to use torch and numpy to speed up the matrix multiplications, but please don't just use their transformer implementation.\n",
        "\n",
        "In the image below, you see the design of one Encoder Block. We want you to set up this Block. Please use your implementation of the Self-Attention (doesn't have to be multi-head) and build the Add & Norm and Feed Forward layers on top of it. Add & Norm and the Feed Forward should be implementations by PyTorch or else. You only need to use your own Self-Attention function.\n",
        "\n",
        "* Show that your model block works, by forwarding a randomly initialized tensor through it once. Print the values of the Random input tensor, the output tensor and the Q,K and V matrices. **(RESULT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UKcg3ziOih-d",
        "outputId": "cf08129e-0b27-4513-c90b-675f9ba078a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" height=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\", height=300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "i_TVEngxgCTh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build self-attention"
      ],
      "metadata": {
        "id": "9z4Wi2xyNpq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iClUJhlrih-d"
      },
      "outputs": [],
      "source": [
        "class self_attention(nn.Module):\n",
        "  def __init__(self, embed_size):\n",
        "    super(self_attention, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.q = nn.Linear(embed_size, embed_size)\n",
        "    self.k = nn.Linear(embed_size, embed_size)\n",
        "    self.v = nn.Linear(embed_size, embed_size)\n",
        "  def forward(self, x):\n",
        "    query = self.q(x)\n",
        "    key = self.k(x)\n",
        "    value = self.v(x)\n",
        "\n",
        "    S= query.matmul(key.transpose(-2,-1))\n",
        "    S = S/(self.embed_size**0.5)\n",
        "    S = torch.softmax(S, dim=-1)\n",
        "    return S.matmul(value), query, key, value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Encoder"
      ],
      "metadata": {
        "id": "V7sZ0MnINvMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder(nn.Module):\n",
        "  def __init__(self, embed_size, lndim):\n",
        "    super(encoder, self).__init__()\n",
        "    self.self_attention = self_attention(embed_size)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "    self.ln1= nn.Linear(embed_size, lndim)\n",
        "    self.ln2= nn.Linear(lndim, embed_size)\n",
        "    self.act= nn.ReLU()\n",
        "  def forward(self, x):\n",
        "      output, Q, K, V = self.self_attention(x)\n",
        "      x= self.norm1(x+output)\n",
        "      x= self.norm2(x+self.ln2(self.act(self.ln1(x))))\n",
        "      return x, Q, K, V\n"
      ],
      "metadata": {
        "id": "evR4KGgGP7UR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "PSjB8j3yN03V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 28, 28)\n",
        "\n",
        "block = encoder(28, 56)\n",
        "\n",
        "out, Q, K, V = block(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"\\nQ:\\n\", Q)\n",
        "print(\"\\nK:\\n\", K)\n",
        "print(\"\\nV:\\n\", V)\n",
        "print(\"\\nOutput:\\n\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K_It7T7gRWM",
        "outputId": "f47c67cc-592c-477c-f8b2-2355da0f9757"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[[ 0.3407, -0.7186,  0.2485,  ...,  0.6806, -0.4684,  0.0272],\n",
            "         [-0.3070,  0.9536,  0.5065,  ..., -0.0046, -1.4808, -0.4607],\n",
            "         [ 0.2990,  0.8637, -0.2279,  ..., -1.8814, -0.1344,  0.1014],\n",
            "         ...,\n",
            "         [-2.2575,  1.3466, -0.7011,  ...,  1.2885,  0.8074, -0.7119],\n",
            "         [ 0.2386,  0.5361,  0.1436,  ...,  0.6470, -0.5483,  1.6475],\n",
            "         [ 0.5114, -0.8427,  0.8347,  ...,  0.4807,  0.4986, -0.6574]],\n",
            "\n",
            "        [[ 1.3771,  0.4581,  1.1626,  ...,  0.6905, -1.7329,  0.2714],\n",
            "         [-0.9778, -0.5344, -1.7767,  ..., -0.7071, -0.0103,  0.5246],\n",
            "         [-0.2135, -0.9023,  0.3432,  ...,  0.9843, -0.7998,  0.8360],\n",
            "         ...,\n",
            "         [-0.6046, -1.9588, -0.6948,  ..., -0.0039,  0.5153, -1.1242],\n",
            "         [ 0.5512, -0.6040,  0.6258,  ...,  0.1885,  0.1005,  0.9686],\n",
            "         [-0.5089,  2.0722, -0.9525,  ...,  0.7120,  2.1778, -1.0791]],\n",
            "\n",
            "        [[ 1.1564,  0.7803, -0.5001,  ...,  0.3236, -1.5500, -2.1761],\n",
            "         [-0.7173, -0.2090,  0.9867,  ...,  1.4410, -0.4948, -0.0323],\n",
            "         [ 0.7926, -0.5427,  0.2001,  ...,  0.8203, -0.2795,  1.1250],\n",
            "         ...,\n",
            "         [-1.2721, -0.7801, -0.4232,  ..., -1.0842,  1.1738, -1.3572],\n",
            "         [ 0.8247,  1.3476, -1.5340,  ...,  1.8599, -0.9016,  1.3899],\n",
            "         [ 1.4462, -0.2162,  1.4636,  ...,  1.0151,  0.0927,  0.1341]]])\n",
            "\n",
            "Q:\n",
            " tensor([[[-0.3425, -0.1959,  0.4883,  ..., -0.2789,  0.0061,  0.4114],\n",
            "         [-0.8528, -0.8532,  0.9126,  ..., -0.9335,  0.3960, -0.1537],\n",
            "         [-0.0515,  0.0255, -0.7623,  ...,  0.0935, -0.1842, -0.0479],\n",
            "         ...,\n",
            "         [ 0.5168,  0.3728, -1.6544,  ..., -0.1256, -0.5543, -0.1212],\n",
            "         [-0.5084, -0.0801,  0.1850,  ..., -1.1123,  1.0681,  0.9432],\n",
            "         [-0.4530,  0.5099,  0.2373,  ..., -0.3392, -1.0658,  0.3489]],\n",
            "\n",
            "        [[-0.9812, -0.9379,  0.0368,  ..., -0.0038, -0.6566,  0.2456],\n",
            "         [ 0.1959, -0.2474, -0.4534,  ...,  0.3397, -0.0093, -0.0021],\n",
            "         [-0.0123,  0.5182,  0.0885,  ...,  0.1458, -0.3991, -0.4030],\n",
            "         ...,\n",
            "         [ 0.0422,  0.4778, -0.3323,  ...,  0.8051,  0.3130, -0.5295],\n",
            "         [-0.4443, -0.2941,  0.2482,  ..., -0.4772,  0.3204,  0.4083],\n",
            "         [-0.5790, -0.5609, -0.1510,  ...,  0.0076,  0.3999,  0.8680]],\n",
            "\n",
            "        [[-0.7031, -0.2312,  1.6216,  ..., -0.7726, -0.3616,  1.0552],\n",
            "         [-0.7911, -0.2327,  0.4578,  ..., -1.7440,  0.4097,  0.4412],\n",
            "         [-0.9053, -0.0480,  0.0625,  ...,  0.0624,  0.3413,  0.8253],\n",
            "         ...,\n",
            "         [ 0.4164, -0.0730,  0.3103,  ...,  0.4786,  0.3105, -0.0924],\n",
            "         [-0.3731, -0.1255, -0.6902,  ..., -0.3677,  0.0982,  0.4674],\n",
            "         [ 0.1463, -0.3515,  0.3211,  ...,  0.0513, -0.0769,  0.0694]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "K:\n",
            " tensor([[[ 0.5717,  0.7365, -0.4458,  ...,  0.5963,  0.9213, -0.5410],\n",
            "         [ 0.7572,  0.1140, -0.7257,  ..., -0.0981, -0.8285, -0.0899],\n",
            "         [-0.6572,  0.4317,  0.0301,  ...,  0.3542, -0.0024,  0.3667],\n",
            "         ...,\n",
            "         [-0.0631,  0.4569,  0.4459,  ..., -0.1567,  0.7588,  0.5322],\n",
            "         [-0.3512,  0.9207, -0.3314,  ...,  0.0759, -0.5182, -0.5939],\n",
            "         [ 1.0308, -0.4012,  0.1096,  ..., -0.4241, -0.9733,  1.1085]],\n",
            "\n",
            "        [[-0.1506,  1.1422, -0.7215,  ...,  0.1713,  0.3207, -0.4587],\n",
            "         [ 0.2457,  0.0836, -1.1701,  ...,  0.2867, -0.8119,  0.5532],\n",
            "         [-0.7067, -0.3393, -0.7853,  ...,  0.2795,  0.4389,  0.0158],\n",
            "         ...,\n",
            "         [-0.0054, -0.7215, -0.6587,  ..., -0.2525, -0.8509,  0.1212],\n",
            "         [-0.0467,  0.5850, -0.0428,  ...,  0.5022, -0.1842, -0.4727],\n",
            "         [ 0.7670,  0.3880,  0.8091,  ..., -0.9801, -0.0161,  0.3726]],\n",
            "\n",
            "        [[ 1.7107, -0.4577,  0.7280,  ..., -0.4881, -0.7068, -0.0030],\n",
            "         [ 0.5136,  0.8729,  1.0676,  ...,  0.0921,  0.2961, -0.4070],\n",
            "         [-0.3223, -0.6132,  0.1090,  ...,  0.2741, -0.2443, -0.3880],\n",
            "         ...,\n",
            "         [ 0.1648,  0.4284, -0.2938,  ...,  0.7014,  0.4781,  0.3094],\n",
            "         [ 0.1754,  0.8786,  0.0096,  ...,  0.2598,  0.9291, -1.4948],\n",
            "         [ 1.0127,  0.1502,  0.3946,  ..., -0.0135,  0.5872, -0.4333]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "V:\n",
            " tensor([[[-0.1446,  0.5305,  0.0459,  ...,  0.1611, -0.4908,  0.2027],\n",
            "         [-1.4542,  0.0827, -0.5893,  ..., -0.2918,  0.1826, -0.0076],\n",
            "         [ 0.6042, -0.4516, -0.5800,  ..., -0.4074,  0.5802, -0.6696],\n",
            "         ...,\n",
            "         [ 0.8724, -0.9457,  0.1768,  ..., -0.0107, -0.5073,  1.0086],\n",
            "         [ 1.3226,  0.8568, -0.0048,  ...,  0.5847, -0.1351, -0.8709],\n",
            "         [-0.1220, -0.5059, -0.5225,  ..., -0.0487, -0.4691,  0.8308]],\n",
            "\n",
            "        [[-0.4915,  0.1485, -0.2238,  ...,  0.0931,  0.4044,  0.3704],\n",
            "         [ 0.1229, -1.5828, -0.2610,  ..., -0.2885,  0.5011,  0.6186],\n",
            "         [ 0.8740, -0.4124, -0.3125,  ...,  1.2583, -0.5610,  0.9664],\n",
            "         ...,\n",
            "         [-0.4049, -0.3573, -0.0205,  ..., -1.1236,  1.0670, -0.8919],\n",
            "         [-0.6631, -0.0532, -0.0979,  ...,  0.3440,  0.2516,  0.2178],\n",
            "         [-0.5415, -0.9802, -1.3343,  ..., -0.0557, -0.2295, -0.0076]],\n",
            "\n",
            "        [[-0.4822,  0.5236,  0.3661,  ...,  0.0811, -0.2801, -0.4117],\n",
            "         [-0.2335,  1.5698,  0.6734,  ..., -0.4571, -0.2345,  0.9491],\n",
            "         [-0.3887, -0.1347,  0.4576,  ..., -0.3391, -0.3089, -0.3403],\n",
            "         ...,\n",
            "         [-0.2520, -0.7553, -0.8310,  ..., -0.2040,  1.3831,  0.6109],\n",
            "         [ 0.8109,  0.1186,  0.9517,  ..., -0.3345, -0.6635, -0.5432],\n",
            "         [-0.4691, -0.6793,  1.0510,  ..., -0.2348,  0.1961,  0.8309]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Output:\n",
            " tensor([[[ 0.3171, -0.7928,  0.0043,  ...,  1.0071, -0.6149,  0.5817],\n",
            "         [-0.0131,  1.2809,  0.4114,  ..., -0.1075, -1.4694, -0.4051],\n",
            "         [ 0.9297,  1.1203, -0.5786,  ..., -2.1209, -0.3109,  0.0525],\n",
            "         ...,\n",
            "         [-2.0673,  1.6618, -0.6983,  ...,  1.1388,  0.4050, -0.9077],\n",
            "         [ 0.6256,  0.6546, -0.0541,  ...,  0.5745, -0.4391,  1.3983],\n",
            "         [ 0.9772, -0.4270,  0.9159,  ...,  0.6100,  0.5378, -0.6576]],\n",
            "\n",
            "        [[ 1.4052,  0.4845,  0.8516,  ...,  0.2114, -2.1316,  0.4221],\n",
            "         [-0.5166, -0.1085, -1.3398,  ..., -0.4988,  0.2520,  0.8021],\n",
            "         [-0.5898, -0.9247, -0.3075,  ...,  0.7611, -1.1253,  0.7989],\n",
            "         ...,\n",
            "         [-0.3733, -1.4307, -0.6332,  ...,  0.0895,  0.8494, -0.6160],\n",
            "         [ 0.7495, -1.1630,  0.2978,  ...,  0.1049,  0.1485,  1.5568],\n",
            "         [-0.6496,  1.8177, -1.1184,  ...,  0.5904,  2.2684, -0.9690]],\n",
            "\n",
            "        [[ 1.2684,  0.5667, -0.3359,  ...,  0.5323, -1.4578, -1.7441],\n",
            "         [-0.5149, -0.1299,  1.0958,  ...,  1.4962, -0.7299,  0.1491],\n",
            "         [ 0.8644, -0.9070,  0.0128,  ...,  0.7952, -0.6817,  1.6041],\n",
            "         ...,\n",
            "         [-1.4503, -0.8850, -0.3145,  ..., -1.0486,  1.1009, -1.0264],\n",
            "         [ 0.0756,  0.9674, -2.0252,  ...,  1.1685, -1.5160,  0.9454],\n",
            "         [ 1.0016, -0.0647,  1.2568,  ...,  0.7161, -0.4268,  0.2257]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMxWT1SCih-e"
      },
      "source": [
        "### Task 11.2 Use your own Transformer Block\n",
        "\n",
        "* Chain 3 of your transformer blocks to set up a model. Put 1 fully connected layer head on top. **(RESULT)**\n",
        "* Train your model on the MNIST dataset for image classification. **(RESULT)**\n",
        "* Report the test accuracy after training. **(RESULT)**\n",
        "\n",
        "Can you make your own attention work? :)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build simple transformer"
      ],
      "metadata": {
        "id": "hlbpGCDTNlyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HHeHEu4Vih-e"
      },
      "outputs": [],
      "source": [
        "class transformer(nn.Module):\n",
        "  def __init__(self, embed_size, lndim):\n",
        "    super(transformer, self).__init__()\n",
        "    self.encoder1= encoder(embed_size, lndim)\n",
        "    self.encoder2= encoder(embed_size, lndim)\n",
        "    self.encoder3= encoder(embed_size, lndim)\n",
        "    self.fc= nn.Linear(embed_size, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=x.squeeze(1)\n",
        "    x,Q, K, V= self.encoder1(x)\n",
        "    x,Q, K, V= self.encoder2(x)\n",
        "    x,Q, K, V= self.encoder3(x)\n",
        "    m= torch.mean(x, dim=1)\n",
        "    return self.fc(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "v5zk-ISNNjrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "  T.Resize(28),\n",
        "  T.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "  root=\"./../datasets\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "  root=\"./../datasets\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader =  torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=32)\n",
        "test_loader =  torch.utils.data.DataLoader(test_set, shuffle=False, batch_size=32)\n"
      ],
      "metadata": {
        "id": "05SjwJ3icLHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2e1e42-f756-451d-fd47-b655c6321645"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.18MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.56MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Training logic"
      ],
      "metadata": {
        "id": "QkxMcyseN3hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, epoch, device):\n",
        "    model.train()\n",
        "    for i in range(epoch):\n",
        "      totalloss=0\n",
        "      for batch_x, batch_y in train_loader:\n",
        "          batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(batch_x)\n",
        "          loss = criterion(output, batch_y)\n",
        "          totalloss+=loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(\"Epoch: \", i, \"Average Training Loss: \", totalloss/len(train_loader))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QrIv3CxMb5A4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Test logic"
      ],
      "metadata": {
        "id": "PDwn5qEZN6hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            output = model(batch_x)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        print(\"Accuracy of the network:\",correct / total)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lk137rbPdbDc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Test on MNIST"
      ],
      "metadata": {
        "id": "mmnhy0QIN-rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= transformer(28, 56)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model, train_loader, optimizer, criterion, 10, device)\n",
        "test(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ3vIGOieD4f",
        "outputId": "973a89c9-b5b7-497f-957d-acd8416bcab4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Average Training Loss:  0.887586820936203\n",
            "Epoch:  1 Average Training Loss:  0.5223461686929067\n",
            "Epoch:  2 Average Training Loss:  0.42644229337771733\n",
            "Epoch:  3 Average Training Loss:  0.37761253734032313\n",
            "Epoch:  4 Average Training Loss:  0.3448488276839256\n",
            "Epoch:  5 Average Training Loss:  0.32116650668382646\n",
            "Epoch:  6 Average Training Loss:  0.30450626935958863\n",
            "Epoch:  7 Average Training Loss:  0.2878247905880213\n",
            "Epoch:  8 Average Training Loss:  0.27475180576841035\n",
            "Epoch:  9 Average Training Loss:  0.2656788719167312\n",
            "Accuracy of the network: 0.9076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgsSLId2ih-e"
      },
      "source": [
        "## Congratz, you made it! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8oGUvnvih-f"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyforecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}